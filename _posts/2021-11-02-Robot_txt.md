---
layout: post
title: "Robot_txt"
date: 2021-11-02 17:21:35
image: '/assets/img/'
description: 'Robot.txt (로봇테스트)'
tags:
- Robots.txt
- crawling
- 로봇
categories:
- crawling
twitter_text: ''
---

## Robots.txt ?

로봇 배제 표준(robots exclusion standard), 로봇 배제 프로토콜(robots exclusion protocol)은 웹 사이트에 로봇이 접근하는 것을 방지하기 위한 규약으로, 일반적으로 __접근 제한__ 에 대한 설명을 robots.txt에 기술한다.

### 지시어 ###

- user-agent: 규칙이 적용 되는 크롤러의 이름
- disallow: 유저 에이전트의 디렉토리 또는 페이지 크롤링을 차단
- allow: 유저 에이전트의 디렉토리 또는 페이지 크롤링을 허용( 글봇에만 적용 가능)
- sitemap: 웹사이트의 모든 리소스를 나열한 목록 파일
크롤링 봇 이름: Googlebot (구글), Yeti (네이버)

① 모든 로봇에게 모든 문서 접근 허용  

---
User-agent: *  
Allow: /

---

② 모든 로봇에게 모든 문서 접근 차단  

---

User-agent: *  
Disallow: /

---

③ 모든 로봇에게 디렉터리 및 특정파일 접근을 차단  

---

User-agent: *  
Disallow: /directory/  
Disallow: /directory/file.html

---  

④ 특정 로봇에 모든 문서 접근 차단  

---

User-agent: Bot_name  
Disallow: /

---  

등등 여러 크롤링 봇을 허용 및 차단 시켜주는 기능이다.

이번에 Gitblog를 작성하면서, 우연히 알게된 정보인데  
내가 작성한 글을 지켜주는 보안이라고 생각이 든다.

