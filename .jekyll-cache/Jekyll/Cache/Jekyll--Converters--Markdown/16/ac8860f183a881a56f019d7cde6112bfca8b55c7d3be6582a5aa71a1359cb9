I"<h2 id="robotstxt-">Robots.txt ?</h2>

<p>로봇 배제 표준(robots exclusion standard), 로봇 배제 프로토콜(robots exclusion protocol)은 웹 사이트에 로봇이 접근하는 것을 방지하기 위한 규약으로,<br />
일반적으로 <strong>접근 제한</strong> 에 대한 설명을 robots.txt에 기술한다.</p>

<h3 id="지시어">지시어</h3>

<ul>
  <li>user-agent: 규칙이 적용 되는 크롤러의 이름</li>
  <li>disallow: 유저 에이전트의 디렉토리 또는 페이지 크롤링을 차단</li>
  <li>allow: 유저 에이전트의 디렉토리 또는 페이지 크롤링을 허용( 글봇에만 적용 가능)</li>
  <li>sitemap: 웹사이트의 모든 리소스를 나열한 목록 파일<br />
크롤링 봇 이름: Googlebot (구글), Yeti (네이버)</li>
</ul>

<p>① 모든 로봇에게 모든 문서 접근 허용</p>

<hr />
<h4 id="user-agent-">User-agent: *</h4>
<h4 id="allow-">Allow: /</h4>

<hr />

<p>② 모든 로봇에게 모든 문서 접근 차단</p>

<hr />

<h4 id="user-agent--1">User-agent: *</h4>
<h4 id="disallow-">Disallow: /</h4>

<hr />

<p>③ 모든 로봇에게 디렉터리 및 특정파일 접근을 차단</p>

<hr />

<h4 id="user-agent--2">User-agent: *</h4>
<h4 id="disallow-directory">Disallow: /directory/</h4>
<h4 id="disallow-directoryfilehtml">Disallow: /directory/file.html</h4>

<hr />

<p>④ 특정 로봇에 모든 문서 접근 차단</p>

<hr />

<h4 id="user-agent-bot_name">User-agent: Bot_name</h4>
<h4 id="disallow--1">Disallow: /</h4>

<hr />

<p>등등 여러 크롤링 봇을 허용 및 차단 시켜주는 기능이다.</p>

<p>이번에 Gitblog를 작성하면서, 우연히 알게된 정보인데<br />
무단 크롤링을 방지하기위해 나중에 사용해야될거같다.</p>

:ET